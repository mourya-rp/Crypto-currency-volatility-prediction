# -*- coding: utf-8 -*-
"""Crptocurrency volatility project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdugML5LA4u9eorWgeD9uC-VYpZtUTkM
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

import warnings
warnings.filterwarnings('ignore')

#Phase 1
#Loading the dataset
df = pd.read_csv("processed_cryptocurrency_data.csv")

df

# 1. Data Cleaning & Consistency
df_clean = df.drop(columns=['Unnamed: 0', 'timestamp'])
df_clean['date'] = pd.to_datetime(df_clean['date'])
df_clean = df_clean.sort_values(by=['crypto_name', 'date']).reset_index(drop=True)

df_clean

# Handle potential missing values (zeros in volume/marketCap)
cols_to_check = ['volume', 'marketCap']
df_clean[cols_to_check] = df_clean[cols_to_check].replace(0, np.nan)
df_clean[cols_to_check] = df_clean.groupby('crypto_name')[cols_to_check].transform(lambda x: x.ffill().bfill())

df_clean

# Phase 2 : Feature Engineering
def engineer_features(group):
    # Daily Returns
    group['daily_return'] = group['close'].pct_change()

    # Target: Rolling Volatility (14-day std dev of returns)
    group['rolling_volatility'] = group['daily_return'].rolling(window=14).std()

    # Liquidity Ratio
    group['liquidity_ratio'] = group['volume'] / group['marketCap']

    # ATR (Average True Range)
    high_low = group['high'] - group['low']
    high_close = np.abs(group['high'] - group['close'].shift())
    low_close = np.abs(group['low'] - group['close'].shift())
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    group['atr'] = tr.rolling(window=14).mean()

    # Bollinger Bands
    sma = group['close'].rolling(window=20).mean()
    std = group['close'].rolling(window=20).std()
    group['bb_upper'] = sma + (2 * std)
    group['bb_lower'] = sma - (2 * std)

    return group

df_engineered = df_clean.groupby('crypto_name').apply(engineer_features).reset_index(drop=True)
df_engineered = df_engineered.dropna() # Drop NaN from rolling windows
df_engineered

# 3. Normalization
numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'marketCap',
                'rolling_volatility', 'liquidity_ratio', 'atr', 'bb_upper', 'bb_lower']

scaler = MinMaxScaler()
df_engineered[numeric_cols] = scaler.fit_transform(df_engineered[numeric_cols])

df_engineered

# Save output
df_engineered.to_csv('processed_cryptocurrency_data.csv', index=False)

#Phase 3 : EXPLORATORY DATA ANALYSIS

# 1. Load the Processed Data
df = pd.read_csv('processed_cryptocurrency_data.csv')

# Set plotting style
plt.style.use('ggplot')

# --- Plot 1: Correlation Heatmap ---
plt.figure(figsize=(12, 10))
# Calculate correlation matrix
corr_matrix = df.corr(numeric_only=True)
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Matrix (Scaled Data)')
plt.show()

# --- Plot 2: Scaled Volatility Distribution ---
plt.figure(figsize=(10, 6))
sns.histplot(df['rolling_volatility'], bins=50, kde=True, color='green')
plt.title('Distribution of Scaled Volatility (Target)')
plt.xlabel('Scaled Volatility (0-1)')
plt.show()

# --- Plot 3: Price vs Volatility Overlay ---
# We filter for Bitcoin to get a clear single-asset view
# (Ensure 'Bitcoin' exists in your processed data 'crypto_name' column)
subset = df[df['crypto_name'] == 'Bitcoin'].sort_values(by='date')

plt.figure(figsize=(14, 7))
plt.plot(pd.to_datetime(subset['date']), subset['close'], label='Scaled Price', color='blue', alpha=0.6)
plt.plot(pd.to_datetime(subset['date']), subset['rolling_volatility'], label='Scaled Volatility', color='orange', alpha=0.9)
plt.title('Bitcoin: Scaled Price vs. Scaled Volatility')
plt.legend()
plt.show()

# --- Plot 4: Liquidity vs Volatility ---
plt.figure(figsize=(10, 6))
# Sample 5000 points to avoid overplotting if dataset is large
sns.scatterplot(data=df.sample(5000, random_state=42),
                x='liquidity_ratio', y='rolling_volatility', alpha=0.3)
plt.title('Liquidity Ratio vs. Volatility')
plt.xlabel('Scaled Liquidity Ratio')
plt.ylabel('Scaled Volatility')
plt.show()

# Phase 4 : Model building and Training

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

# 1. Load the Processed Data
print("Loading data...")
df = pd.read_csv('processed_cryptocurrency_data.csv')

# Ensure data is sorted by date for time-series splitting
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values(by=['date', 'crypto_name'])

# 2. Feature Selection
# We drop non-numeric identifiers and the target variable itself
target = 'rolling_volatility'
ignore_cols = ['date', 'crypto_name', target, 'daily_return']
features = [col for col in df.columns if col not in ignore_cols]

X = df[features]
y = df[target]

# 3. Time-Series Train-Test Split (80/20 split)
# We do NOT shuffle because future data shouldn't leak into the past
split_index = int(len(df) * 0.8)

X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

print(f"Training on {len(X_train)} samples, Testing on {len(X_test)} samples.")

# 4. Model Training
print("Training models...")

# Model A: Linear Regression (Baseline)
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_preds = lr_model.predict(X_test)

# Model B: Random Forest (Advanced)
rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# 5. Evaluation Function
def print_metrics(model_name, y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"\n--- {model_name} Performance ---")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE:  {mae:.4f}")
    print(f"R²:   {r2:.4f}")

print_metrics("Linear Regression", y_test, lr_preds)
print_metrics("Random Forest", y_test, rf_preds)

# 6. Save the Best Model
# We'll save Random Forest as it's generally more robust for deployment
joblib.dump(rf_model, 'volatility_model.pkl')
print("\nModel saved to 'volatility_model.pkl'")

# 7. Feature Importance Plot
importances = rf_model.feature_importances_
feature_df = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_df = feature_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_df['Feature'], feature_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()
plt.title('Feature Importance (Random Forest)')
plt.xlabel('Importance')
plt.show()

# Hyper parameter tuning

import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score

# 1. Load Data
print("Loading data...")
df = pd.read_csv('processed_cryptocurrency_data.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values(by=['date', 'crypto_name'])

# 2. Setup Features
target = 'rolling_volatility'
ignore_cols = ['date', 'crypto_name', target, 'daily_return']
features = [col for col in df.columns if col not in ignore_cols]

X = df[features]
y = df[target]

# 3. SPLIT DATA
# We use the standard split logic
split_idx = int(len(df) * 0.8)
X_train_full = X.iloc[:split_idx]
y_train_full = y.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_test = y.iloc[split_idx:]

# --- SPEED OPTIMIZATION HERE ---
# Instead of tuning on 50,000+ rows, we tune on the last 10,000 rows of training data.
# This gives the model enough recent context to find good parameters without wasting time.
tune_size = 10000
if len(X_train_full) > tune_size:
    print(f"Subsampling last {tune_size} rows for faster tuning...")
    X_tune = X_train_full.iloc[-tune_size:]
    y_tune = y_train_full.iloc[-tune_size:]
else:
    X_tune = X_train_full
    y_tune = y_train_full

# 4. Randomized Search (Faster than Grid Search)
param_dist = {
    'n_estimators': [50, 100, 150],       # Limit max trees
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']      # Test different feature selection methods
}

tscv = TimeSeriesSplit(n_splits=3)

rf = RandomForestRegressor(random_state=42, n_jobs=-1)

random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=10,                            # LIMIT: Only try 10 random combinations
    cv=tscv,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

print("Starting Randomized Search (Fast Mode)...")
random_search.fit(X_tune, y_tune)

print(f"\nBest Parameters Found: {random_search.best_params_}")

# 5. Final Training
# Now that we found the best params quickly, we train ONE final model on the FULL training set
print("Training final model with best parameters on full history...")
best_model = random_search.best_estimator_
best_model.fit(X_train_full, y_train_full)

# 6. Evaluate
preds = best_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
r2 = r2_score(y_test, preds)

print(f"Final Optimized RMSE: {rmse:.4f}")
print(f"Final Optimized R²: {r2:.4f}")

# Save
joblib.dump(best_model, 'optimized_volatility_model.pkl')
print("Model saved!")









